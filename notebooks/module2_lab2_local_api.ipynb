{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - LLM API Lab 2 - Local\n",
    "## Pre-requisites\n",
    "- software to run local LLM installed/running\n",
    "    - eg. ollama or LM Studio\n",
    "- LLM download to local machine\n",
    "- LLM running locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                         ID              SIZE      MODIFIED    \n",
      "deepseek-r1:1.5b             a42b25d8c10a    1.1 GB    2 hours ago    \n",
      "llama3.2:1b-instruct-q2_K    3718017cfd4e    580 MB    7 days ago     \n"
     ]
    }
   ],
   "source": [
    "# ollama - check models downloaded \n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      PROCESSOR    UNTIL              \n",
      "deepseek-r1:1.5b    a42b25d8c10a    2.1 GB    100% GPU     6 seconds from now    \n"
     ]
    }
   ],
   "source": [
    "# ollama - check LLM running\n",
    "!ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke API\n",
    "> note:\n",
    "> - using OpenAI \n",
    ">   - though we are running ollama with Deepseek !\n",
    "> - api_key \n",
    ">   - mandatory argument\n",
    ">   - in reality, not needed as LLM is local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, the user just asked for a joke. I should respond with something light and fun.\n",
      "\n",
      "Hmm, maybe a classic one? Well, don't want it to be too risquÃ©, I guess.\n",
      "\n",
      "How about this: \"Why do people love clock towers so much? Because they keep time...\" \n",
      "\n",
      "That should work! It ties the metaphor of hands on a clock or watch to relationships.\n",
      "</think>\n",
      "\n",
      "Why do people love clock towers so much?  \n",
      "Because they keep time, and everyone knows right!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"not-needed\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"deepseek-r1:1.5b\", \n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"tell me a joke\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-709',\n",
       " 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\\nAlright, the user just asked for a joke. I should respond with something light and fun.\\n\\nHmm, maybe a classic one? Well, don\\'t want it to be too risquÃ©, I guess.\\n\\nHow about this: \"Why do people love clock towers so much? Because they keep time...\" \\n\\nThat should work! It ties the metaphor of hands on a clock or watch to relationships.\\n</think>\\n\\nWhy do people love clock towers so much?  \\nBecause they keep time, and everyone knows right!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))],\n",
       " 'created': 1739166067,\n",
       " 'model': 'deepseek-r1:1.5b',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': 'fp_ollama',\n",
       " 'usage': CompletionUsage(completion_tokens=104, prompt_tokens=7, total_tokens=111, completion_tokens_details=None, prompt_tokens_details=None),\n",
       " '_request_id': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'stop',\n",
       " 'index': 0,\n",
       " 'logprobs': None,\n",
       " 'message': ChatCompletionMessage(content=\"<think>\\n\\n</think>\\n\\nSure! Here's a light-hearted joke for you:\\n\\nWhy donâ€™t skeletons fight each other?  \\nBecause they donâ€™t have the *guts*! ðŸ˜„\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(completion.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"<think>\\n\\n</think>\\n\\nSure! Here's a light-hearted joke for you:\\n\\nWhy donâ€™t skeletons fight each other?  \\nBecause they donâ€™t have the *guts*! ðŸ˜„\",\n",
       " 'refusal': None,\n",
       " 'role': 'assistant',\n",
       " 'audio': None,\n",
       " 'function_call': None,\n",
       " 'tool_calls': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
